# IoT Cryptojacking Detection and Adversarial Robustness Analysis

## 1\. Project Overview

This project aims to develop and evaluate machine learning methodologies for the detection of cryptojacking malware in Internet of Things (IoT) networks. The research focuses on analyzing network traffic behavior using statistical time-window features. A core objective of this work is to compare the performance and security robustness of a classic **Random Forest (RF)** classifier against a **Deep Learning (Multi-Layer Perceptron)** model when subjected to adversarial evasion attacks.

## 2\. Methodology

The pipeline consists of three main stages: Data Engineering, Model Training, and Adversarial Stress Testing.

### 2.1. Data Engineering

The dataset is derived from network traffic captures (PCAP files) generated by IoT devices. The raw data is processed to extract behavioral features rather than payload inspection, ensuring privacy and lightweight processing.

  * **Feature Extraction:** Traffic is aggregated into sliding windows of 10 packets.
  * **Computed Features:** For each window, statistical metrics are calculated, including Mean and Variance of Inter-arrival Times (`Time_Mean`, `Time_Var`) and Packet Lengths (`Length_Mean`, `Length_Var`).
  * **Data Splitting:** To prevent data leakage and ensure scientific validity, the dataset is split into:
      * **Dev Set (90%):** Used for training and validation (stratified split).
      * **Holdout Set (10%):** A strictly isolated "Caveau" dataset used exclusively for final testing and adversarial generation.

### 2.2. Models Implemented

Two distinct architectures were implemented to assess the trade-off between computational efficiency and robustness:

1.  **Random Forest (RF):**

      * An ensemble of decision trees chosen for its interpretability and efficiency on tabular data.
      * Achieved high baseline accuracy and precision in standard conditions.

2.  **Deep Learning (MLP):**

      * A Feed-Forward Neural Network implemented in PyTorch.
      * **Architecture:** Input Layer $\rightarrow$ Dense(64) $\rightarrow$ Dense(32) $\rightarrow$ Output(1).
      * **Optimization:** Includes `WeightedRandomSampler` to handle class imbalance, `Dropout` for regularization, and `EarlyStopping` to prevent overfitting.

### 2.3. Adversarial Robustness Analysis

A significant portion of this research is dedicated to evaluating the models under hostile conditions. We implemented a suite of **Black-Box Adversarial Attacks** designed to simulate realistic evasion techniques used by malware authors:

  * **Timing Jitter:** Injection of random delays in packet transmission to disrupt temporal patterns (simulating `sleep()` calls).
  * **Packet Padding:** Addition of dummy bytes to payloads to alter dimensional statistics (Obfuscation).
  * **Dummy Traffic Injection (Mimicry):** Injection of benign-like traffic statistics to dilute the malicious behavioral footprint.

## 3\. Repository Structure

/
├── attacks/                # Adversarial generation and evaluation suite
│   └── advers_attack.py    # Script for generating attacks and testing robustness
├── data/                   # Contains processed datasets (dev_set.csv, holdout_dataset.csv)
├── models/                 # Serialized models (.pkl for RF, .pth for DL) and scalers
├── results/                # Output logs and generated plots
│   └── adversarial_robustness.png
├── src/                    # Core source code for training and data processing
│   ├── build_features.py   # Feature extraction pipeline
│   ├── train_rf.py         # Random Forest training script
│   └── train_dl.py         # Deep Learning training script
└── README.md

## 4\. Usage Instructions

### Prerequisites

Ensure all dependencies are installed via `pip`:

```bash
pip install pandas numpy scikit-learn torch matplotlib seaborn joblib
```

### Execution Order

The scripts must be executed in the following order to ensure data consistency:

1.  **Data Preparation:**
    ```bash
    python3 src/build_features.py
    ```
2.  **Train Random Forest:**
    (This script also generates the train/holdout split)
    ```bash
    python3 src/train_rf.py
    ```
3.  **Train Deep Learning:**
    ```bash
    python3 src/train_dl.py
    ```
4.  **Run Adversarial Robustness Test:**
    ```bash
    python3 src/advers_attack.py
    ```

## 5\. Experimental Results

The comparative analysis yielded significant findings regarding the trade-off between precision and security:

  * **Baseline Performance:** The Random Forest model demonstrated slightly superior performance on clean data, achieving a Recall of **\~92%** compared to the Deep Learning model's **\~90%**.
  * **Adversarial Robustness:**
      * **Random Forest Fragility:** The RF model proved highly sensitive to **Packet Padding** attacks, with detection rates dropping significantly (Recall decrease \> 20%). This indicates a reliance on rigid threshold-based rules.
      * **Deep Learning Resilience:** The MLP model exhibited superior generalization. It remained virtually unaffected by **Timing Jitter** and **Mimicry** attacks (Recall drop \~0%), and demonstrated greater resistance to Padding compared to the RF.

### Conclusion

While Random Forest offers efficiency for static environments, **Deep Learning provides a critical advantage in adversarial settings**, maintaining operational stability against active evasion attempts.


-----

**Author:** Andreina Datti
**Course:** Machine Learning Security
**Institution:** Sapienza University of Rome